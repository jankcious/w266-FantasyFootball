{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Position</th>\n",
       "      <th>Year</th>\n",
       "      <th>Week</th>\n",
       "      <th>Points</th>\n",
       "      <th>SeasonPoints</th>\n",
       "      <th>AvgSeasonPoints</th>\n",
       "      <th>AvgWkPoints</th>\n",
       "      <th>Diff_from_Avg</th>\n",
       "      <th>...</th>\n",
       "      <th>max_date</th>\n",
       "      <th>min_date</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>name</th>\n",
       "      <th>news</th>\n",
       "      <th>team</th>\n",
       "      <th>news_clean</th>\n",
       "      <th>news_unigrams</th>\n",
       "      <th>news_bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>NO</td>\n",
       "      <td>QB</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>37.7</td>\n",
       "      <td>338.5</td>\n",
       "      <td>19.911765</td>\n",
       "      <td>21.15625</td>\n",
       "      <td>17.788235</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-09-15</td>\n",
       "      <td>2016-09-08</td>\n",
       "      <td>2016-09-11 04:33:00</td>\n",
       "      <td>Drew Brees completed 28-of-42 passes for 424 y...</td>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>\\n             Aside from an early lost fumble...</td>\n",
       "      <td>Saints</td>\n",
       "      <td>Aside early lost fumble self played quarterbac...</td>\n",
       "      <td>['asid', 'earli', 'lost', 'fumbl', 'self', 'pl...</td>\n",
       "      <td>['asid earli', 'the highlight', 'bumbl defens'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>NO</td>\n",
       "      <td>QB</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>37.7</td>\n",
       "      <td>338.5</td>\n",
       "      <td>19.911765</td>\n",
       "      <td>21.15625</td>\n",
       "      <td>17.788235</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-09-15</td>\n",
       "      <td>2016-09-08</td>\n",
       "      <td>2016-09-08 11:56:00</td>\n",
       "      <td>Drew Brees got a $30 million signing bonus on ...</td>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>\\n             That bonus will be spread out a...</td>\n",
       "      <td>Saints</td>\n",
       "      <td>That bonus spread across next five years ownte...</td>\n",
       "      <td>['bonu', 'spread', 'across', 'next', 'five', '...</td>\n",
       "      <td>['that bonu', 'the entir', 'across next', 'bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>NO</td>\n",
       "      <td>QB</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>338.5</td>\n",
       "      <td>19.911765</td>\n",
       "      <td>21.15625</td>\n",
       "      <td>-5.411765</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>2016-09-15</td>\n",
       "      <td>2016-09-18 05:32:00</td>\n",
       "      <td>Drew Brees completed 29-of-44 passes for 263 y...</td>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>\\n             Brees got off to a very slow st...</td>\n",
       "      <td>Saints</td>\n",
       "      <td>self got slow start averaging five yards per a...</td>\n",
       "      <td>['self', 'got', 'slow', 'start', 'averag', 'fi...</td>\n",
       "      <td>['atlanta next', 'giant put', 'he didnt', 'it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>NO</td>\n",
       "      <td>QB</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>25.9</td>\n",
       "      <td>338.5</td>\n",
       "      <td>19.911765</td>\n",
       "      <td>21.15625</td>\n",
       "      <td>5.988235</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-09-29</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>2016-09-27 12:01:00</td>\n",
       "      <td>Drew Brees completed 36-of-54 passes for 376 y...</td>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>\\n             Unfortunately for the Saints, t...</td>\n",
       "      <td>Saints</td>\n",
       "      <td>Unfortunately ownteam defense couldnt stop Fal...</td>\n",
       "      <td>['unfortun', 'ownteam', 'defens', 'couldnt', '...</td>\n",
       "      <td>['cobi fleener', 'diego week', 'fleener two', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>NO</td>\n",
       "      <td>QB</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>11.8</td>\n",
       "      <td>338.5</td>\n",
       "      <td>19.911765</td>\n",
       "      <td>21.15625</td>\n",
       "      <td>-8.111765</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-10-06</td>\n",
       "      <td>2016-09-29</td>\n",
       "      <td>2016-10-02 08:25:00</td>\n",
       "      <td>Drew Brees completed 23-of-36 passes for 206 y...</td>\n",
       "      <td>Drew Brees</td>\n",
       "      <td>\\n             It was not a vintage performanc...</td>\n",
       "      <td>Saints</td>\n",
       "      <td>It vintage performance self He struggled accur...</td>\n",
       "      <td>['vintag', 'perform', 'self', 'struggl', 'accu...</td>\n",
       "      <td>['charger gift', 'it vintag', 'panther home', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Player Team Position  Year  Week  Points  SeasonPoints  \\\n",
       "0  Drew Brees   NO       QB  2016     1    37.7         338.5   \n",
       "1  Drew Brees   NO       QB  2016     1    37.7         338.5   \n",
       "2  Drew Brees   NO       QB  2016     2    14.5         338.5   \n",
       "3  Drew Brees   NO       QB  2016     3    25.9         338.5   \n",
       "4  Drew Brees   NO       QB  2016     4    11.8         338.5   \n",
       "\n",
       "   AvgSeasonPoints  AvgWkPoints  Diff_from_Avg  \\\n",
       "0        19.911765     21.15625      17.788235   \n",
       "1        19.911765     21.15625      17.788235   \n",
       "2        19.911765     21.15625      -5.411765   \n",
       "3        19.911765     21.15625       5.988235   \n",
       "4        19.911765     21.15625      -8.111765   \n",
       "\n",
       "                         ...                           max_date   min_date  \\\n",
       "0                        ...                         2016-09-15 2016-09-08   \n",
       "1                        ...                         2016-09-15 2016-09-08   \n",
       "2                        ...                         2016-09-22 2016-09-15   \n",
       "3                        ...                         2016-09-29 2016-09-22   \n",
       "4                        ...                         2016-10-06 2016-09-29   \n",
       "\n",
       "                 date                                           headline  \\\n",
       "0 2016-09-11 04:33:00  Drew Brees completed 28-of-42 passes for 424 y...   \n",
       "1 2016-09-08 11:56:00  Drew Brees got a $30 million signing bonus on ...   \n",
       "2 2016-09-18 05:32:00  Drew Brees completed 29-of-44 passes for 263 y...   \n",
       "3 2016-09-27 12:01:00  Drew Brees completed 36-of-54 passes for 376 y...   \n",
       "4 2016-10-02 08:25:00  Drew Brees completed 23-of-36 passes for 206 y...   \n",
       "\n",
       "         name                                               news    team  \\\n",
       "0  Drew Brees  \\n             Aside from an early lost fumble...  Saints   \n",
       "1  Drew Brees  \\n             That bonus will be spread out a...  Saints   \n",
       "2  Drew Brees  \\n             Brees got off to a very slow st...  Saints   \n",
       "3  Drew Brees  \\n             Unfortunately for the Saints, t...  Saints   \n",
       "4  Drew Brees  \\n             It was not a vintage performanc...  Saints   \n",
       "\n",
       "                                          news_clean  \\\n",
       "0  Aside early lost fumble self played quarterbac...   \n",
       "1  That bonus spread across next five years ownte...   \n",
       "2  self got slow start averaging five yards per a...   \n",
       "3  Unfortunately ownteam defense couldnt stop Fal...   \n",
       "4  It vintage performance self He struggled accur...   \n",
       "\n",
       "                                       news_unigrams  \\\n",
       "0  ['asid', 'earli', 'lost', 'fumbl', 'self', 'pl...   \n",
       "1  ['bonu', 'spread', 'across', 'next', 'five', '...   \n",
       "2  ['self', 'got', 'slow', 'start', 'averag', 'fi...   \n",
       "3  ['unfortun', 'ownteam', 'defens', 'couldnt', '...   \n",
       "4  ['vintag', 'perform', 'self', 'struggl', 'accu...   \n",
       "\n",
       "                                        news_bigrams  \n",
       "0  ['asid earli', 'the highlight', 'bumbl defens'...  \n",
       "1  ['that bonu', 'the entir', 'across next', 'bas...  \n",
       "2  ['atlanta next', 'giant put', 'he didnt', 'it ...  \n",
       "3  ['cobi fleener', 'diego week', 'fleener two', ...  \n",
       "4  ['charger gift', 'it vintag', 'panther home', ...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../data/news_and_scores.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    START_TOKEN = \"<s>\"\n",
    "    END_TOKEN = \"</s>\"\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "    def __init__(self, tokens, size=None):\n",
    "        self.unigram_counts = collections.Counter(tokens)\n",
    "        self.num_unigrams = sum(self.unigram_counts.values())\n",
    "        # leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "        top_counts = self.unigram_counts.most_common(None if size is None else (size - 3))\n",
    "        vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
    "                 [w for w,c in top_counts])\n",
    "\n",
    "        # Assign an id to each word, by frequency\n",
    "        self.id_to_word = dict(enumerate(vocab))\n",
    "        self.word_to_id = {v:k for k,v in self.id_to_word.items()}\n",
    "        self.size = len(self.id_to_word)\n",
    "        if size is not None:\n",
    "            assert(self.size <= size)\n",
    "\n",
    "        # For convenience\n",
    "        self.wordset = set(self.word_to_id.keys())\n",
    "\n",
    "        # Store special IDs\n",
    "        self.START_ID = self.word_to_id[self.START_TOKEN]\n",
    "        self.END_ID = self.word_to_id[self.END_TOKEN]\n",
    "        self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "    def words_to_ids(self, words):\n",
    "        return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "    def ids_to_words(self, ids):\n",
    "        return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "    def sentence_to_ids(self, words):\n",
    "        return [self.START_ID] + self.words_to_ids(words) + [self.END_ID]\n",
    "\n",
    "    def ordered_words(self):\n",
    "        \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "        return self.ids_to_words(range(self.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_feed_unigram = (word for news in data['news_unigrams'] for word in literal_eval(news))\n",
    "token_feed_bigram = (word for news in data['news_bigrams'] for word in literal_eval(news))\n",
    "\n",
    "vocab_unigram = Vocabulary(token_feed_unigram)\n",
    "vocab_bigram = Vocabulary(token_feed_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9009"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_unigram.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156134"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_bigram.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"self\": 22735\n",
      "\"week\": 10076\n",
      "\"yard\": 6303\n",
      "\"game\": 6273\n",
      "\"play\": 4594\n",
      "\"ownteam\": 4360\n",
      "\"touchdown\": 3528\n",
      "\"first\": 2665\n",
      "\"get\": 2491\n",
      "\"back\": 2469\n"
     ]
    }
   ],
   "source": [
    "for word, count in vocab_unigram.unigram_counts.most_common(10):\n",
    "    print(\"\\\"%s\\\": %d\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_unigram = [collections.Counter(vocab_unigram.words_to_ids(x)) for x in data['news_unigrams']]\n",
    "encoded_bigram = [collections.Counter(vocab_bigram.words_to_ids(x)) for x in data['news_bigrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_unigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(z)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_unigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_unigram), vocab_unigram.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_bigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(z)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_bigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_bigram), vocab_bigram.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_unigram = pd.DataFrame([collections.Counter(vocab_unigram.words_to_ids(x)) for x in data['news_unigrams']], \n",
    "#                        columns = range(vocab_unigram.size))\n",
    "# encoded_unigram.fillna(value= 0, inplace=True)\n",
    "# encoded_bigram = pd.DataFrame([collections.Counter(vocab_bigram.words_to_ids(x)) for x in data['news_bigrams']], \n",
    "#                        columns = range(vocab_bigram.size))\n",
    "# encoded_bigram.fillna(value= 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoded_unigram.fillna(value= 0, inplace=True)\n",
    "# encoded_bigram.fillna(value= 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_uni_points, X_test_uni_points, y_train_uni_points, y_test_uni_points = \\\n",
    "        train_test_split(encoded_unigram_sparse, data['Points'], test_size = 0.25)\n",
    "X_train_bi_points, X_test_bi_points, y_train_bi_points, y_test_bi_points = \\\n",
    "        train_test_split(encoded_bigram_sparse, data['Points'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Raw unigrams score was 6.612167 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams against points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_uni_points, y_train_uni_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.8545310196534768"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_uni_points) - np.array(y_test_uni_points, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams against points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_bi_points, y_train_bi_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.3142876885748294"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_bi_points) - np.array(y_test_bi_points, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams against avg points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_uni_avg, X_test_uni_avg, y_train_uni_avg, y_test_uni_avg = \\\n",
    "        train_test_split(encoded_unigram_sparse, data['Diff_from_Avg'], test_size = 0.25)\n",
    "X_train_bi_avg, X_test_bi_avg, y_train_bi_avg, y_test_bi_avg = \\\n",
    "        train_test_split(encoded_bigram_sparse, data['Diff_from_Avg'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_uni_avg, y_train_uni_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9864342353352633"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_uni_avg) - np.array(y_test_uni_avg, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams against avg points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_bi_avg, y_train_bi_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9645152896373732"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_bi_avg) - np.array(y_test_bi_avg, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams against weekly avg points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Player', 'Team', 'Position', 'Year', 'Week', 'Points', 'SeasonPoints',\n",
       "       'AvgSeasonPoints', 'AvgWkPoints', 'Diff_from_Avg', 'Diff_from_WkAvg',\n",
       "       'max_date', 'min_date', 'date', 'headline', 'name', 'news', 'team',\n",
       "       'news_clean', 'news_unigrams', 'news_bigrams', 'Ratio_from_Avg',\n",
       "       'Ratio_from_WkAvg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_uni_wavg, X_test_uni_wavg, y_train_uni_wavg, y_test_uni_wavg = train_test_split(\n",
    "            encoded_unigram_sparse, data['Diff_from_WkAvg'], test_size = 0.25)\n",
    "X_train_bi_wavg, X_test_bi_wavg, y_train_bi_wavg, y_test_bi_wavg = train_test_split(\n",
    "            encoded_bigram_sparse, data['Diff_from_WkAvg'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_uni_wavg, y_train_uni_wavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5213611770729187"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_uni_wavg) - np.array(y_test_uni_wavg, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams against weekly avg points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=10000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=10000, random_state = 42)\n",
    "SVM.fit(X_train_bi_wavg, y_train_bi_wavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.6534714273003077"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_bi_wavg) - np.array(y_test_bi_wavg, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio from season average as a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Ratio_from_Avg'] = data['Points'] / data['AvgSeasonPoints']\n",
    "data['Ratio_from_WkAvg'] = data['Points'] / data['AvgWkPoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_uni_ratio, X_test_uni_ratio, y_train_uni_ratio, y_test_uni_ratio = \\\n",
    "        train_test_split(encoded_unigram_sparse, data['Ratio_from_Avg'], test_size = 0.5)\n",
    "X_train_bi_ratio, X_test_bi_ratio, y_train_bi_ratio, y_test_bi_ratio = \\\n",
    "        train_test_split(encoded_bigram_sparse, data['Ratio_from_Avg'], test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_uni_ratio, y_train_uni_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0704991428840449"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_uni_ratio) - np.array(y_test_uni_ratio, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_bi_ratio, y_train_bi_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0523903426339332"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_bi_ratio) - np.array(y_test_bi_ratio, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio from Weekly Average as a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_uni_wkratio, X_test_uni_wkratio, y_train_uni_wkratio, y_test_uni_wkratio = \\\n",
    "        train_test_split(encoded_unigram_sparse, data['Ratio_from_WkAvg'], test_size = 0.25)\n",
    "X_train_bi_wkratio, X_test_bi_wkratio, y_train_bi_wkratio, y_test_bi_wkratio = \\\n",
    "        train_test_split(encoded_bigram_sparse, data['Ratio_from_WkAvg'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_uni_wkratio, y_train_uni_wkratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7262377173674246"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_uni_wkratio) - np.array(y_test_uni_wkratio, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_bi_wkratio, y_train_bi_wkratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74001396145068854"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_bi_wkratio) - np.array(y_test_bi_wkratio, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8775351,  0.8775351,  0.8775351, ...,  0.8775351,  0.8775351,\n",
       "        0.8775351])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM.predict(X_test_bi_wkratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode n-grams as present (instead of count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_unigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(1)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_unigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_unigram), vocab_unigram.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_bigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(1)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_bigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_bigram), vocab_bigram.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_unigram_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun above code to replicate with boolean indicators instead of counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2777"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8775351  0.8775351  0.8775351 ...,  0.8775351  0.8775351  0.8775351]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
