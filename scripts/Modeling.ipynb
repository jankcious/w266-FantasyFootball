{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Position</th>\n",
       "      <th>Year</th>\n",
       "      <th>Week</th>\n",
       "      <th>Points</th>\n",
       "      <th>SeasonPoints</th>\n",
       "      <th>AvgSeasonPoints</th>\n",
       "      <th>AvgWkPoints</th>\n",
       "      <th>Diff_from_Avg</th>\n",
       "      <th>...</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>name</th>\n",
       "      <th>news</th>\n",
       "      <th>team</th>\n",
       "      <th>news_clean</th>\n",
       "      <th>news_unigrams</th>\n",
       "      <th>news_bigrams</th>\n",
       "      <th>orig_unigrams</th>\n",
       "      <th>orig_bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Johnson</td>\n",
       "      <td>ARI</td>\n",
       "      <td>RB</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>27.52</td>\n",
       "      <td>323.05</td>\n",
       "      <td>20.190625</td>\n",
       "      <td>21.536667</td>\n",
       "      <td>7.329375</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-11-13 07:43:00</td>\n",
       "      <td>David Johnson rushed 19 times for 55 yards and...</td>\n",
       "      <td>David Johnson</td>\n",
       "      <td>\\n             Even in what could be classifie...</td>\n",
       "      <td>Cardinals</td>\n",
       "      <td>Even could classified underwhelming game self ...</td>\n",
       "      <td>['even', 'could', 'classifi', 'underwhelm', 'g...</td>\n",
       "      <td>['both came', 'even could', 'rb play', 'big ca...</td>\n",
       "      <td>['even', 'could', 'classifi', 'underwhelm', 'g...</td>\n",
       "      <td>['35 rush', 'both came', \"cardin '\", 'midway t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>David Johnson</td>\n",
       "      <td>ARI</td>\n",
       "      <td>RB</td>\n",
       "      <td>2016</td>\n",
       "      <td>13</td>\n",
       "      <td>25.62</td>\n",
       "      <td>323.05</td>\n",
       "      <td>20.190625</td>\n",
       "      <td>21.536667</td>\n",
       "      <td>5.429375</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-12-04 07:44:00</td>\n",
       "      <td>David Johnson rushed 18 times for 84 yards and...</td>\n",
       "      <td>David Johnson</td>\n",
       "      <td>\\n             There is not much left to say a...</td>\n",
       "      <td>Cardinals</td>\n",
       "      <td>There much left say self self A dominant force...</td>\n",
       "      <td>['much', 'left', 'say', 'self', 'self', 'domin...</td>\n",
       "      <td>['a domin', 'rb everi', 'the fact', 'there muc...</td>\n",
       "      <td>['much', 'left', 'say', 'david', 'johnson', '....</td>\n",
       "      <td>['a domin', 'rb1 everi', 'the fact', 'about da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Johnson</td>\n",
       "      <td>ARI</td>\n",
       "      <td>RB</td>\n",
       "      <td>2016</td>\n",
       "      <td>14</td>\n",
       "      <td>25.23</td>\n",
       "      <td>323.05</td>\n",
       "      <td>20.190625</td>\n",
       "      <td>21.536667</td>\n",
       "      <td>5.039375</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-12-11 05:45:00</td>\n",
       "      <td>David Johnson rushed 20 times for 80 yards and...</td>\n",
       "      <td>David Johnson</td>\n",
       "      <td>\\n             It was not the best performance...</td>\n",
       "      <td>Cardinals</td>\n",
       "      <td>It best performance self lost fumble early gam...</td>\n",
       "      <td>['best', 'perform', 'self', 'lost', 'fumbl', '...</td>\n",
       "      <td>['dolphin defens', 'he also', 'it best', 'also...</td>\n",
       "      <td>['best', 'perform', 'johnson', ',', 'lost', 'f...</td>\n",
       "      <td>['- two', '100 total', 'dolphin defens', 'he a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Johnson</td>\n",
       "      <td>ARI</td>\n",
       "      <td>RB</td>\n",
       "      <td>2016</td>\n",
       "      <td>15</td>\n",
       "      <td>25.78</td>\n",
       "      <td>323.05</td>\n",
       "      <td>20.190625</td>\n",
       "      <td>21.536667</td>\n",
       "      <td>5.589375</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-12-18 08:18:00</td>\n",
       "      <td>David Johnson rushed 12 times for 53 yards and...</td>\n",
       "      <td>David Johnson</td>\n",
       "      <td>\\n             Johnson was somewhat quiet in t...</td>\n",
       "      <td>Cardinals</td>\n",
       "      <td>self somewhat quiet first half gaining  yards ...</td>\n",
       "      <td>['self', 'somewhat', 'quiet', 'first', 'half',...</td>\n",
       "      <td>['christma eve', 'eve toptwo', 'it might', 'ke...</td>\n",
       "      <td>['johnson', 'somewhat', 'quiet', 'first', 'hal...</td>\n",
       "      <td>['14 game', 'christma eve', 'it might', 'kerwy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>David Johnson</td>\n",
       "      <td>ARI</td>\n",
       "      <td>RB</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>24.09</td>\n",
       "      <td>323.05</td>\n",
       "      <td>20.190625</td>\n",
       "      <td>21.536667</td>\n",
       "      <td>3.899375</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-11-28 02:24:00</td>\n",
       "      <td>David Johnson dislocated his finger in Sunday'...</td>\n",
       "      <td>David Johnson</td>\n",
       "      <td>\\n             Johnson checked out a few times...</td>\n",
       "      <td>Cardinals</td>\n",
       "      <td>self checked times second half Sunday ultimate...</td>\n",
       "      <td>['self', 'check', 'time', 'second', 'half', 's...</td>\n",
       "      <td>['hell like', 'rb redskin', 'sunday ultim', 'c...</td>\n",
       "      <td>['johnson', 'check', 'time', 'second', 'half',...</td>\n",
       "      <td>[\"' ll\", ', but', '- three', '161 yard', '21 t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Player Team Position  Year  Week  Points  SeasonPoints  \\\n",
       "0  David Johnson  ARI       RB  2016    10   27.52        323.05   \n",
       "1  David Johnson  ARI       RB  2016    13   25.62        323.05   \n",
       "2  David Johnson  ARI       RB  2016    14   25.23        323.05   \n",
       "3  David Johnson  ARI       RB  2016    15   25.78        323.05   \n",
       "4  David Johnson  ARI       RB  2016    12   24.09        323.05   \n",
       "\n",
       "   AvgSeasonPoints  AvgWkPoints  Diff_from_Avg  \\\n",
       "0        20.190625    21.536667       7.329375   \n",
       "1        20.190625    21.536667       5.429375   \n",
       "2        20.190625    21.536667       5.039375   \n",
       "3        20.190625    21.536667       5.589375   \n",
       "4        20.190625    21.536667       3.899375   \n",
       "\n",
       "                         ...                                        date  \\\n",
       "0                        ...                         2016-11-13 07:43:00   \n",
       "1                        ...                         2016-12-04 07:44:00   \n",
       "2                        ...                         2016-12-11 05:45:00   \n",
       "3                        ...                         2016-12-18 08:18:00   \n",
       "4                        ...                         2016-11-28 02:24:00   \n",
       "\n",
       "                                            headline           name  \\\n",
       "0  David Johnson rushed 19 times for 55 yards and...  David Johnson   \n",
       "1  David Johnson rushed 18 times for 84 yards and...  David Johnson   \n",
       "2  David Johnson rushed 20 times for 80 yards and...  David Johnson   \n",
       "3  David Johnson rushed 12 times for 53 yards and...  David Johnson   \n",
       "4  David Johnson dislocated his finger in Sunday'...  David Johnson   \n",
       "\n",
       "                                                news       team  \\\n",
       "0  \\n             Even in what could be classifie...  Cardinals   \n",
       "1  \\n             There is not much left to say a...  Cardinals   \n",
       "2  \\n             It was not the best performance...  Cardinals   \n",
       "3  \\n             Johnson was somewhat quiet in t...  Cardinals   \n",
       "4  \\n             Johnson checked out a few times...  Cardinals   \n",
       "\n",
       "                                          news_clean  \\\n",
       "0  Even could classified underwhelming game self ...   \n",
       "1  There much left say self self A dominant force...   \n",
       "2  It best performance self lost fumble early gam...   \n",
       "3  self somewhat quiet first half gaining  yards ...   \n",
       "4  self checked times second half Sunday ultimate...   \n",
       "\n",
       "                                       news_unigrams  \\\n",
       "0  ['even', 'could', 'classifi', 'underwhelm', 'g...   \n",
       "1  ['much', 'left', 'say', 'self', 'self', 'domin...   \n",
       "2  ['best', 'perform', 'self', 'lost', 'fumbl', '...   \n",
       "3  ['self', 'somewhat', 'quiet', 'first', 'half',...   \n",
       "4  ['self', 'check', 'time', 'second', 'half', 's...   \n",
       "\n",
       "                                        news_bigrams  \\\n",
       "0  ['both came', 'even could', 'rb play', 'big ca...   \n",
       "1  ['a domin', 'rb everi', 'the fact', 'there muc...   \n",
       "2  ['dolphin defens', 'he also', 'it best', 'also...   \n",
       "3  ['christma eve', 'eve toptwo', 'it might', 'ke...   \n",
       "4  ['hell like', 'rb redskin', 'sunday ultim', 'c...   \n",
       "\n",
       "                                       orig_unigrams  \\\n",
       "0  ['even', 'could', 'classifi', 'underwhelm', 'g...   \n",
       "1  ['much', 'left', 'say', 'david', 'johnson', '....   \n",
       "2  ['best', 'perform', 'johnson', ',', 'lost', 'f...   \n",
       "3  ['johnson', 'somewhat', 'quiet', 'first', 'hal...   \n",
       "4  ['johnson', 'check', 'time', 'second', 'half',...   \n",
       "\n",
       "                                        orig_bigrams  \n",
       "0  ['35 rush', 'both came', \"cardin '\", 'midway t...  \n",
       "1  ['a domin', 'rb1 everi', 'the fact', 'about da...  \n",
       "2  ['- two', '100 total', 'dolphin defens', 'he a...  \n",
       "3  ['14 game', 'christma eve', 'it might', 'kerwy...  \n",
       "4  [\"' ll\", ', but', '- three', '161 yard', '21 t...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../data/news_and_scores.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Player', 'Team', 'Position', 'Year', 'Week', 'Points', 'SeasonPoints',\n",
       "       'AvgSeasonPoints', 'AvgWkPoints', 'Diff_from_Avg', 'Diff_from_WkAvg',\n",
       "       'max_date', 'min_date', 'date', 'headline', 'name', 'news', 'team',\n",
       "       'news_clean', 'news_unigrams', 'news_bigrams', 'orig_unigrams',\n",
       "       'orig_bigrams'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    START_TOKEN = \"<s>\"\n",
    "    END_TOKEN = \"</s>\"\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "    def __init__(self, tokens, size=None):\n",
    "        self.unigram_counts = collections.Counter(tokens)\n",
    "        self.num_unigrams = sum(self.unigram_counts.values())\n",
    "        # leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "        top_counts = self.unigram_counts.most_common(None if size is None else (size - 3))\n",
    "        vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
    "                 [w for w,c in top_counts])\n",
    "\n",
    "        # Assign an id to each word, by frequency\n",
    "        self.id_to_word = dict(enumerate(vocab))\n",
    "        self.word_to_id = {v:k for k,v in self.id_to_word.items()}\n",
    "        self.size = len(self.id_to_word)\n",
    "        if size is not None:\n",
    "            assert(self.size <= size)\n",
    "\n",
    "        # For convenience\n",
    "        self.wordset = set(self.word_to_id.keys())\n",
    "\n",
    "        # Store special IDs\n",
    "        self.START_ID = self.word_to_id[self.START_TOKEN]\n",
    "        self.END_ID = self.word_to_id[self.END_TOKEN]\n",
    "        self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "    def words_to_ids(self, words):\n",
    "        return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "    def ids_to_words(self, ids):\n",
    "        return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "    def sentence_to_ids(self, words):\n",
    "        return [self.START_ID] + self.words_to_ids(words) + [self.END_ID]\n",
    "\n",
    "    def ordered_words(self):\n",
    "        \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "        return self.ids_to_words(range(self.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_feed_unigram = (word for news in data['news_unigrams'] for word in literal_eval(news))\n",
    "token_feed_bigram = (word for news in data['news_bigrams'] for word in literal_eval(news))\n",
    "\n",
    "vocab_unigram = Vocabulary(token_feed_unigram)\n",
    "vocab_bigram = Vocabulary(token_feed_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7978"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_unigram.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118140"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_bigram.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"self\": 15193\n",
      "\"week\": 7082\n",
      "\"yard\": 4353\n",
      "\"game\": 4334\n",
      "\"ownteam\": 3069\n",
      "\"play\": 3062\n",
      "\"touchdown\": 2504\n",
      "\"first\": 1822\n",
      "\"get\": 1742\n",
      "\"pass\": 1671\n"
     ]
    }
   ],
   "source": [
    "for word, count in vocab_unigram.unigram_counts.most_common(10):\n",
    "    print(\"\\\"%s\\\": %d\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_unigram = [collections.Counter(vocab_unigram.words_to_ids(x)) for x in data['news_unigrams']]\n",
    "encoded_bigram = [collections.Counter(vocab_bigram.words_to_ids(x)) for x in data['news_bigrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_unigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(z)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_unigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_unigram), vocab_unigram.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_bigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(z)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_bigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_bigram), vocab_bigram.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_unigram = pd.DataFrame([collections.Counter(vocab_unigram.words_to_ids(x)) for x in data['news_unigrams']], \n",
    "#                        columns = range(vocab_unigram.size))\n",
    "# encoded_unigram.fillna(value= 0, inplace=True)\n",
    "# encoded_bigram = pd.DataFrame([collections.Counter(vocab_bigram.words_to_ids(x)) for x in data['news_bigrams']], \n",
    "#                        columns = range(vocab_bigram.size))\n",
    "# encoded_bigram.fillna(value= 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoded_unigram.fillna(value= 0, inplace=True)\n",
    "# encoded_bigram.fillna(value= 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_uni_points, X_test_uni_points, y_train_uni_points, y_test_uni_points = \\\n",
    "        train_test_split(encoded_unigram_sparse, data['Points'], test_size = 0.25)\n",
    "X_train_bi_points, X_test_bi_points, y_train_bi_points, y_test_bi_points = \\\n",
    "        train_test_split(encoded_bigram_sparse, data['Points'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Raw unigrams score was 6.612167 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams against points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_uni_points, y_train_uni_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.8545310196534768"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_uni_points) - np.array(y_test_uni_points, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams against points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_bi_points, y_train_bi_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.3142876885748294"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_bi_points) - np.array(y_test_bi_points, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams against avg points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_uni_avg, X_test_uni_avg, y_train_uni_avg, y_test_uni_avg = \\\n",
    "        train_test_split(encoded_unigram_sparse, data['Diff_from_Avg'], test_size = 0.25)\n",
    "X_train_bi_avg, X_test_bi_avg, y_train_bi_avg, y_test_bi_avg = \\\n",
    "        train_test_split(encoded_bigram_sparse, data['Diff_from_Avg'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_uni_avg, y_train_uni_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9864342353352633"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_uni_avg) - np.array(y_test_uni_avg, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams against avg points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_bi_avg, y_train_bi_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9645152896373732"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_bi_avg) - np.array(y_test_bi_avg, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams against weekly avg points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Player', 'Team', 'Position', 'Year', 'Week', 'Points', 'SeasonPoints',\n",
       "       'AvgSeasonPoints', 'AvgWkPoints', 'Diff_from_Avg', 'Diff_from_WkAvg',\n",
       "       'max_date', 'min_date', 'date', 'headline', 'name', 'news', 'team',\n",
       "       'news_clean', 'news_unigrams', 'news_bigrams', 'Ratio_from_Avg',\n",
       "       'Ratio_from_WkAvg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_uni_wavg, X_test_uni_wavg, y_train_uni_wavg, y_test_uni_wavg = train_test_split(\n",
    "            encoded_unigram_sparse, data['Diff_from_WkAvg'], test_size = 0.25)\n",
    "X_train_bi_wavg, X_test_bi_wavg, y_train_bi_wavg, y_test_bi_wavg = train_test_split(\n",
    "            encoded_bigram_sparse, data['Diff_from_WkAvg'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_uni_wavg, y_train_uni_wavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5213611770729187"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_uni_wavg) - np.array(y_test_uni_wavg, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams against weekly avg points with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=10000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=10000, random_state = 42)\n",
    "SVM.fit(X_train_bi_wavg, y_train_bi_wavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.6534714273003077"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_bi_wavg) - np.array(y_test_bi_wavg, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio from season average as a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Ratio_from_Avg'] = data['Points'] / data['AvgSeasonPoints']\n",
    "data['Ratio_from_WkAvg'] = data['Points'] / data['AvgWkPoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_uni_ratio, X_test_uni_ratio, y_train_uni_ratio, y_test_uni_ratio = \\\n",
    "        train_test_split(encoded_unigram_sparse, data['Ratio_from_Avg'], test_size = 0.5)\n",
    "X_train_bi_ratio, X_test_bi_ratio, y_train_bi_ratio, y_test_bi_ratio = \\\n",
    "        train_test_split(encoded_bigram_sparse, data['Ratio_from_Avg'], test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_uni_ratio, y_train_uni_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0704991428840449"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_uni_ratio) - np.array(y_test_uni_ratio, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_bi_ratio, y_train_bi_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0523903426339332"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_bi_ratio) - np.array(y_test_bi_ratio, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio from Weekly Average as a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_uni_wkratio, X_test_uni_wkratio, y_train_uni_wkratio, y_test_uni_wkratio = \\\n",
    "        train_test_split(encoded_unigram_sparse, data['Ratio_from_WkAvg'], test_size = 0.25)\n",
    "X_train_bi_wkratio, X_test_bi_wkratio, y_train_bi_wkratio, y_test_bi_wkratio = \\\n",
    "        train_test_split(encoded_bigram_sparse, data['Ratio_from_WkAvg'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_uni_wkratio, y_train_uni_wkratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7262377173674246"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_uni_wkratio) - np.array(y_test_uni_wkratio, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=20000,\n",
       "     random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=20000, random_state = 42)\n",
    "SVM.fit(X_train_bi_wkratio, y_train_bi_wkratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74001396145068854"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(SVM.predict(X_test_bi_wkratio) - np.array(y_test_bi_wkratio, dtype = np.float32)))**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8775351,  0.8775351,  0.8775351, ...,  0.8775351,  0.8775351,\n",
       "        0.8775351])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM.predict(X_test_bi_wkratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode n-grams as present (instead of count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_unigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(1)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_unigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_unigram), vocab_unigram.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_bigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(1)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_bigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_bigram), vocab_bigram.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_unigram_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun above code to replicate with boolean indicators instead of counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR()\n",
    "params = {}\n",
    "grid = GridSearchCV(SVM, params)\n",
    "grid.fit(encoded_unigram_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR()\n",
    "params = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "        'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid = GridSearchCV(SVM, params, cv=5)\n",
    "grid.fit(encoded_unigram_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.31786175116647236"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=5000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = LinearSVR(max_iter=5000)\n",
    "params = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "        'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid2 = GridSearchCV(SVM, params, cv=5)\n",
    "grid2.fit(encoded_bigram_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.015749675526160627"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000.0, 'epsilon': 0.0005}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize\n",
    "token_feed_unigram = (word for news in data['news_unigrams'] for word in literal_eval(news))\n",
    "token_feed_bigram = (word for news in data['news_bigrams'] for word in literal_eval(news))\n",
    "vocab_unigram = Vocabulary(token_feed_unigram)\n",
    "vocab_bigram = Vocabulary(token_feed_bigram)\n",
    "\n",
    "token_feed_unigram_orig = (word for news in data['orig_unigrams'] for word in literal_eval(news))\n",
    "token_feed_bigram_orig = (word for news in data['orig_bigrams'] for word in literal_eval(news))\n",
    "vocab_unigram_orig = Vocabulary(token_feed_unigram_orig)\n",
    "vocab_bigram_orig = Vocabulary(token_feed_bigram_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding\n",
    "encoded_unigram = [collections.Counter(vocab_unigram.words_to_ids(x)) for x in data['news_unigrams']]\n",
    "encoded_bigram = [collections.Counter(vocab_bigram.words_to_ids(x)) for x in data['news_bigrams']]\n",
    "encoded_unigram_orig = [collections.Counter(vocab_unigram.words_to_ids(x)) for x in data['orig_unigrams']]\n",
    "encoded_bigram_orig = [collections.Counter(vocab_bigram.words_to_ids(x)) for x in data['orig_bigrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sparse matrix\n",
    "#Processed Unigram\n",
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_unigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(z)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)\n",
    "encoded_unigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_unigram), vocab_unigram.size))\n",
    "#Processed Bigram\n",
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_bigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(z)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)\n",
    "encoded_bigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_bigram), vocab_bigram.size))\n",
    "\n",
    "#Original Unigram\n",
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_unigram_orig):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(z)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)\n",
    "encoded_unigram_orig_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_unigram_orig), vocab_unigram_orig.size))\n",
    "\n",
    "#Original Bigram\n",
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_bigram_orig):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(z)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)\n",
    "encoded_bigram_orig_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_bigram_orig), vocab_bigram_orig.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=5000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unigram Counts No preprocessing\n",
    "SVM = LinearSVR(max_iter=5000)\n",
    "params = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "        'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid = GridSearchCV(SVM, params, cv=5)\n",
    "grid.fit(encoded_unigram_orig_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=5000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Brigram Counts No Preprocessing\n",
    "SVM = LinearSVR(max_iter=5000)\n",
    "params = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "        'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid2 = GridSearchCV(SVM, params, cv=5)\n",
    "grid2.fit(encoded_bigram_orig_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=5000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unigram Counts With preprocessing\n",
    "SVM = LinearSVR(max_iter=5000)\n",
    "params = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "        'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid3 = GridSearchCV(SVM, params, cv=5)\n",
    "grid3.fit(encoded_unigram_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=5000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Brigram Counts With Preprocessing\n",
    "SVM = LinearSVR(max_iter=5000)\n",
    "params = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "        'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid4 = GridSearchCV(SVM, params, cv=5)\n",
    "grid4.fit(encoded_bigram_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reencode all for presence instead of counts\n",
    "#Processed Unigram\n",
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_unigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(1)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)\n",
    "encoded_unigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_unigram), vocab_unigram.size))\n",
    "#Processed Bigram\n",
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_bigram):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(1)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)\n",
    "encoded_bigram_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_bigram), vocab_bigram.size))\n",
    "\n",
    "#Original Unigram\n",
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_unigram_orig):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(1)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)\n",
    "encoded_unigram_orig_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_unigram_orig), vocab_unigram_orig.size))\n",
    "\n",
    "#Original Bigram\n",
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "for x, count in enumerate(encoded_bigram_orig):\n",
    "    for y, z in count.items():\n",
    "        row.append(x)\n",
    "        col.append(y)\n",
    "        val.append(1)\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val = np.array(val)\n",
    "encoded_bigram_orig_sparse = csr_matrix((val, (row, col)), shape=(len(encoded_bigram_orig), vocab_bigram_orig.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=5000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unigram Presence No preprocessing\n",
    "SVM = LinearSVR(max_iter=5000)\n",
    "params = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "        'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid5 = GridSearchCV(SVM, params, cv=5)\n",
    "grid5.fit(encoded_unigram_orig_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=5000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Brigram Presence No Preprocessing\n",
    "SVM = LinearSVR(max_iter=5000)\n",
    "params = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "        'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid6 = GridSearchCV(SVM, params, cv=5)\n",
    "grid6.fit(encoded_bigram_orig_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=5000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unigram Presence With preprocessing\n",
    "SVM = LinearSVR(max_iter=5000)\n",
    "params = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "        'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid7 = GridSearchCV(SVM, params, cv=5)\n",
    "grid7.fit(encoded_unigram_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=5000,\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1000.0, 5000.0, 10000.0, 50000.0, 100000.0], 'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Brigram Presence With Preprocessing\n",
    "SVM = LinearSVR(max_iter=5000)\n",
    "params = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "        'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid8 = GridSearchCV(SVM, params, cv=5)\n",
    "grid8.fit(encoded_bigram_sparse, data['Diff_from_WkAvg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3514815422224595"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(grid.predict(encoded_unigram_orig_sparse) - np.array(data['Diff_from_WkAvg'], dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2581432159521442"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(grid2.predict(encoded_bigram_orig_sparse) - np.array(data['Diff_from_WkAvg'], dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7073729211807604"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(grid3.predict(encoded_unigram_sparse) - np.array(data['Diff_from_WkAvg'], dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2604581541548598"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(grid4.predict(encoded_bigram_sparse) - np.array(data['Diff_from_WkAvg'], dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.315361947454214"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(grid5.predict(encoded_unigram_orig_sparse) - np.array(data['Diff_from_WkAvg'], dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2609544423649139"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(grid6.predict(encoded_bigram_orig_sparse) - np.array(data['Diff_from_WkAvg'], dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5472200281412976"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(grid7.predict(encoded_unigram_sparse) - np.array(data['Diff_from_WkAvg'], dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2689219898415121"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(grid8.predict(encoded_bigram_sparse) - np.array(data['Diff_from_WkAvg'], dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
